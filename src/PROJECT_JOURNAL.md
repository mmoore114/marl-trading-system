# Project Journal & Status: MARL Trading System

## 1. Project Objective
To build a sophisticated, professional-grade Multi-Agent Reinforcement Learning (MARL) system for trading a large universe of U.S. equities. The system is designed with a hierarchical architecture, featuring a team of "specialist agents" that report to a central "Super-Agent" for final portfolio allocation and risk management.

---

## 2. System Architecture & Strategy
The core architecture is a **Hierarchical MARL system** built with Ray RLlib.

* **Specialist Agents (The Analysts)**: A team of agents, each an expert in a specific strategy (e.g., `technical`, `fundamental`, `sentiment`, `risk`). Each specialist analyzes the market and produces a set of recommended actions.
* **Super-Agent (The Portfolio Manager)**: A single, higher-level agent (`portfolio_manager`) that takes the market state and the specialists' recommendations as its input. It performs **strategy allocation**, learning to weigh the advice of its specialists to construct a final, risk-managed portfolio.

---

## 3. Project File Structure & Purpose

### Root Directory
* **`.gitignore`**: Specifies files Git should ignore (e.g., `.venv`, data files, secrets).
* **`config.yaml`**: The central control panel for the project. Defines tickers, dates, train/val/test splits, factor definitions, and model hyperparameters.
* **`.env`**: A secure, local-only file for storing secret API keys.
* **`requirements.txt`**: An explicit list of all Python libraries required for the project.

### `/data`
* `/raw`: Holds raw, unprocessed CSV data downloaded by `data_pipeline.py`.
* `/processed`: Holds feature-rich Parquet files generated by `feature_engineering.py`. This is the direct input for the RL environment.

### `/models`
* `/ray_checkpoints`: Contains saved model checkpoints from Ray RLlib training runs.

### `/reports`
* Contains output files from the backtester, including `summary.txt`, `backtest_equity_curve.csv`, and `quantstats_report.html`.

### `/src`
* **`data_pipeline.py`**: Downloads raw market data.
* **`feature_engineering.py`**: The "factor engine." Calculates technical indicators.
* **`environment.py`**: Defines the custom `TradingEnv` as a Ray RLlib `MultiAgentEnv`, handling state, actions, and rewards.
* **`train.py`**: The main training script. Uses Ray RLlib and the PPO algorithm to train the multi-agent system on a GPU.
* **`backtester.py`**: Evaluates a trained agent checkpoint on the test dataset and generates performance reports.
* **`PROJECT_JOURNAL.md`**: This file.

---

## 4. Current Status & Key Accomplishments (August 25, 2025)
The end-to-end pipeline is now fully functional. A baseline model has been successfully trained and backtested.

* **Environment Setup**: Successfully configured a local VS Code environment to connect to a remote Vertex AI (T4 GPU) instance via Remote-SSH.
* **Python Environment**: Created and managed a dedicated Python virtual environment (`.venv`), resolving all dependency issues and conflicts with the VM's native conda environments.
* **Data Pipeline**: Successfully executed `data_pipeline.py` and `feature_engineering.py` to download and process market data for the initial ticker universe.
* **Model Training**: Successfully trained a multi-agent PPO model using `train.py`. The process was debugged to correctly allocate CPU and GPU resources on the VM.
* **Backtesting**: Successfully evaluated the best-trained checkpoint using `backtester.py` to generate a performance summary on the unseen test dataset.

---

## 5. Detailed Troubleshooting Log
This project required extensive debugging to align the code with the modern Ray RLlib API and the cloud VM environment. Key issues and their solutions are documented below for future reference.

* **SSH Connection**:
    * **Problem**: `Permission denied (publickey)` errors when connecting VS Code to the VM.
    * **Solution**: Used `gcloud compute ssh` to auto-generate and configure the correct SSH keys. Updated the VS Code SSH config file to explicitly point to the `google_compute_engine` identity file. Also addressed the ephemeral IP issue by promoting it to a static IP in the Google Cloud Console.

* **Python Environment & Imports**:
    * **Problem**: `ModuleNotFoundError` and silent script exits.
    * **Solution**: Corrected run commands to use the `-m` flag (e.g., `python -m src.train`) to enable proper module resolution. Ensured the correct `(.venv)` was active and isolated from conflicting conda environments (`base`, `jupyterlab`) by using `conda deactivate`.

* **Dependencies**:
    * **Problem**: `pip install` failed due to syntax errors; `ImportError` during script execution.
    * **Solution**: Corrected syntax in `requirements.txt` (comments on separate lines). Added missing packages (`ta`, `quantstats`) to `requirements.txt` and re-installed.

* **Ray RLlib API & Bugs**:
    * **Problem**: Numerous `AttributeError`, `TypeError`, and `ValueError` exceptions during training and backtesting due to recent updates in the Ray RLlib library.
    * **Solution**: Performed a major overhaul of `train.py`, `environment.py`, and `backtester.py` to conform to the modern RLlib API stack. Key changes included:
        * Using `PPOConfig()` with the builder pattern.
        * Updating resource methods (`.resources()`, `.learners()`, `.env_runners()`).
        * Defining observation/action spaces as `gymnasium.spaces.Dict`.
        * Correcting the `policy_mapping_fn` signature.
        * Replacing the deprecated `compute_actions` with the new `RLModule.get_module()` and `forward_inference` workflow in the backtester.
        * Correcting a bug in the backtester that used the full action distribution output (mean + std) instead of just the mean.

* **Resource Allocation**:
    * **Problem**: Training was stuck in a `PENDING` state.
    * **Solution**: Diagnosed that the requested CPU count (1 driver + 1 eval + 4 workers = 6) exceeded the VM's available 4 CPUs. Solved by reducing `n_envs` to 2 in `config.yaml`. The GPU allocation was also fixed to ensure the learner process utilized the T4 GPU.

---

## 6. Immediate Next Steps
The baseline agent is not profitable, which is expected. The focus now shifts from engineering to quantitative research and iteration.

1.  **Analyze Backtest Report**: Download and analyze the `quantstats_report.html` to understand the specific weaknesses of the baseline strategy.
2.  **Iterate on Model**: Begin the core research loop:
    * **Add More Data**: Expand the training period in `config.yaml`.
    * **Feature Engineering**: Add/remove/modify factors in `config.yaml` and `feature_engineering.py`.
    * **Reward Shaping**: Adjust the reward penalties in `config.yaml` to encourage different agent behaviors.
    * **Hyperparameter Tuning**: Use a tool like Optuna to systematically find better model settings than the defaults.
3.  **Integrate New Data**: Once the EODHD package is available, proceed with `sentiment_pipeline.py` and add fundamental data to the feature engineering process.