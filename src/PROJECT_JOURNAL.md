# Project Journal & Status: MARL Trading System

## 1. Project Objective
To build a sophisticated, professional-grade Multi-Agent Reinforcement Learning (MARL) system for trading a large universe of U.S. equities. The system is designed with a hierarchical architecture, featuring a team of "specialist agents" that report to a central "Super-Agent" for final portfolio allocation and risk management.

---

## 2. System Architecture & Strategy
The core architecture's end goal is a **Hierarchical MARL system**. However, to ensure a robust foundation, the current implementation is a **Single-Agent Baseline**. This involves a single PPO agent responsible for the entire portfolio management task. This baseline allows for rapid iteration and validation of the data pipeline, feature set, reward function, and backtesting framework before introducing the complexity of multiple agents.

---

## 3. Project File Structure & Purpose

### Root Directory
* **`.gitignore`**: Specifies files Git should ignore.
* **`config.yaml`**: The central control panel for the project. Defines tickers (list or file), dates, train/val/test splits, factors, and model hyperparameters.
* **`.env`**: Stores the secret EODHD API key.
* **`requirements.txt`**: An explicit, version-pinned list of all required Python libraries.
* **`get_tickers.py`**: A utility script to fetch the S&P 500 ticker list.

### `/data`
* `/raw`: Holds raw CSV data downloaded by `data_pipeline.py`.
* `/processed`: Holds feature-rich Parquet files generated by `feature_engineering.py`.
* `/cache`: Stores pre-computed NumPy arrays of the entire dataset for memory-efficient loading by the RL environment.

### `/models`
* `/ray_results`: Contains saved model checkpoints from Ray RLlib training runs.

### `/reports`
* Contains backtest outputs: `summary.txt`, `backtest_equity_curve.csv`, and `quantstats_report.html`.

### `/src`
* **`data_pipeline.py`**: Downloads and adjusts raw market data for a large universe from EODHD.
* **`feature_engineering.py`**: The "factor engine." Calculates technical indicators and creates the final processed data.
* **`environment.py`**: Defines the custom `SingleAgentTradingEnv`, which is memory-efficient (using caching and memory-mapping) and handles state, actions, and rewards for large-scale training.
* **`train.py`**: The main training script. Uses a stable version of Ray RLlib and the PPO algorithm to train a single agent on a GPU and save the resulting model checkpoint.
* **`backtester.py`**: Loads a trained agent checkpoint, evaluates it on the test dataset, and generates detailed performance reports using QuantStats.
* **`PROJECT_JOURNAL.md`**: This file.

---

## 4. Current Status & Key Accomplishments (August 31, 2025)
The project has successfully transitioned from a small-scale prototype to a functional, large-scale, end-to-end single-agent pipeline. A baseline model has been trained on the S&P 500 and evaluated.

* **Stable Environment**: Resolved a complex series of dependency conflicts by pinning `ray[rllib]`, `numpy`, `pyarrow`, and other libraries to known-stable versions, creating a reproducible environment.
* **Scaled Data Pipeline**: The data pipeline now successfully downloads, adjusts, and processes data for the entire S&P 500 (~500 stocks).
* **Memory-Efficient RL Environment**: Re-architected `environment.py` to handle large datasets by creating a NumPy cache on disk and using memory-mapping (`mmap`). This solved all "Out of Memory" (OOM) errors.
* **Successful Large-Scale Training**: Successfully trained a baseline PPO agent on the full S&P 500 dataset.
* **Backtesting & Evaluation**: The `backtester.py` script is fully functional. The first baseline model was evaluated and showed profitable performance on the unseen test set, establishing a solid benchmark for future experiments.
* **First Research Loop Completed**: Successfully completed one full iteration of the quantitative research loop: analyzed the baseline, formed a hypothesis to reduce drawdown, modified a reward parameter (`lambda_tc_bps`), re-trained, and re-evaluated the new model.

---

## 5. Detailed Troubleshooting Log
This phase required extensive debugging of library incompatibilities and scaling issues.
* **Dependency Conflicts**: Solved numerous `pip` dependency conflicts by pinning library versions in `requirements.txt`, most notably `ray[rllib]==2.10.0`, `numpy==1.26.4`, and `pyarrow==16.1.0`. Also fixed a hidden dependency issue with `quantstats` requiring `ipython`.
* **Ray RLlib API**: Navigated significant challenges with the new vs. old RLlib API stacks. Settled on a stable, legacy-API-based configuration (`ray==2.10.0`) to resolve persistent metric reporting (`nan`) and crashing errors.
* **Scaling Issues**:
    * **Out of Memory (OOM)**: Solved by re-architecting the environment to cache the full dataset to disk as NumPy arrays and load them with `mmap`.
    * **Data Availability**: Solved `ValueError: Not enough data` by making the environment robustly filter out tickers with insufficient data history and handle non-overlapping date ranges.

---

## 6. Immediate Next Steps
The project is now fully in the quantitative research phase. The immediate goal is to continue iterating on the single-agent model to improve its performance.

1.  **Analyze Latest Experiment**: Analyze the results of the `lambda_tc_bps` experiment to see its effect on drawdown, returns, and turnover.
2.  **Formulate New Hypothesis**: Based on the analysis, form a new hypothesis for improvement (e.g., tune a different reward parameter, add new features).
3.  **Continue Research Loop**: Execute the next train-backtest-analyze cycle.
4.  **Upgrade to Multi-Agent**: Once the single-agent model is sufficiently refined, begin "Phase 2" by upgrading the environment and training scripts to the full hierarchical MARL system.